{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working on a large dataset with many features, stochastic gradient descent for regression works very well. This is an iterative algorithm, which iteratively includes additional data points from your dataset in order to find the best regression coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least-Angle regression, or LARS.an algorithm that works very well when you have a small size of datasets, but with many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean square error regression may have this problem where you overfit a model such that it performs well in training, but poorly in the real world. Mitigating overfitting is an important aspect of building ML models. \n",
    "\n",
    "We can use  regularization to mitigate overfitting and regression models that use this technique to make better, more robust models. There are three regularized regression models that you can build and train using scikit-learn, lasso, ridge, and the elastic net regression models. \n",
    "\n",
    "\n",
    "Lasso and ridge regression differ only in the penalty term that they apply to the objective function, and elastic net is simply a combination of lasso and ridge.\n",
    "\n",
    "we can move beyond regularized regression models to other techniques that you can use for regression, such as support vector machines, nearest neighbors regression, stochastic gradient descent regression, decision trees, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHOOSING REGRESSION ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../files/Capture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a matrix representation of a number of features along rows and size of the dataset along columns. So we've divided our dataset sizes into three broad categories, small, medium, and large. And the number of features can be few, moderate or many. \n",
    "\n",
    "As you can see, these are fairly coarse distinctions, but these are useful in figuring out which estimator is the right one to use. \n",
    "\n",
    "\n",
    "if you have more than 100, 000 data points, you choose to go with the stochastic gradient descent. The SGD algorithm works very well on very large datasets which have a large number of features. \n",
    "\n",
    "If the size of the data that you have to train your model is small, but you have many features, that is there are more features than samples in your dataset, you'll go with the least-angle regression, or LARS. \n",
    "\n",
    "\n",
    "For medium-sized datasets with a moderate number of features, you may have many features initially, but only a few of those features are useful, the lasso and elastic net regression models are what you might find the best. \n",
    "\n",
    "\n",
    "The lasso model tends to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features. The lasso model prefers those features which it considers more significant and tries to eliminate the others. \n",
    "\n",
    "\n",
    "If you have a medium-sized dataset and most of the features in your data are useful, that's when you choose to go with the ridge regression model. \n",
    "\n",
    "\n",
    "If the dataset that you're working with are fairly small and you have a moderate number of features, support vector regression with a linear kernel might help. This also works for medium-sized data where there is non-linearity in the dataset itself. \n",
    "\n",
    "If you have very small data with nonlinearity, you might choose to use support vector regression with the RBF kernel. \n",
    "\n",
    "\n",
    "If you have just a few features and a medium-sized dataset, decision trees and ensemble methods work well. Ensemble methods are when you have many big models which come together to build a more robust model. \n",
    "\n",
    "\n",
    "And finally, the size of your dataset is large and you have just a few features, ordinary least squares regression, that is the very basic regression model might work best. \n",
    "\n",
    "\n",
    "These are all the options available to you to build regression models in scikit-learn. T\n",
    "\n",
    "his little matrix here shows us a quick look up of when you would choose to use which regression model. This can be the starting point for choosing your regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
