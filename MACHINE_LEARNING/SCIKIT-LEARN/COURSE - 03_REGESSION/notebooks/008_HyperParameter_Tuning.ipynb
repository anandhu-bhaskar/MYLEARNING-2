{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are model configuration properties that define a model and they remain constant during the training of the model. \n",
    "\n",
    "\n",
    "An easier way to think of hyperparameters is that they are part of the model design. \n",
    "\n",
    "It's something that you specify that makes up the design of your model. \n",
    "\n",
    "When you talk about a machine learning model, there are typically three bits of data that you associate with the model, \n",
    "\n",
    "- model inputs, which is what you use to train the model; \n",
    "\n",
    "\n",
    "- model parameters, these are what you're trying to figure out during model training; and finally, \n",
    "\n",
    "\n",
    "- model hyperparameters, which make up the design of your model. \n",
    "\n",
    "\n",
    "Model inputs refer to the training data, this is the data that your model uses to learn. You feed in training data during the training process of your model, and this is what the model uses to find the model parameters.\n",
    "\n",
    "\n",
    "The model parameters here are the regression coefficients in a regression model. Based on whether you have a regression model or a classification model, model parameters will of course be different. \n",
    "\n",
    "\n",
    "\n",
    "In the case of regression, the coefficient and the intercept are your model parameters. And finally, you have model hyperparameters. \n",
    "\n",
    "\n",
    "These are the configuration properties for your model. If it's a decision tree model, the depth of the decision tree could be a hyperparameter. \n",
    "\n",
    "\n",
    "If it's a regularized regression model that you are building, the value of alpha that you use to multiply the penalty function is a hyperparameter. \n",
    "\n",
    "\n",
    "The best way to distinguish model parameters and hyperparameters is that model parameters learn or change during the training process, model hyperparameters remain constant. That's something you specify up front. \n",
    "\n",
    "\n",
    "Hyperparameter tuning in scikit-learn is performed using grid search. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../files/Capture14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where you specify all possible values for a hyperparameter, all possible values for alpha, say. \n",
    "\n",
    "All of these values form a grid where every cell is a candidate model, a model that you want to test. So every cell is a particular design of a model that you want to evaluate. \n",
    "\n",
    "You will then use GridSearchCV, where CV stands for cross-validation, to evaluate each candidate model. cross-validation is a technique that you use to evaluate models. \n",
    "\n",
    "Once you use GridSearchCV, scikit-learn is responsible for ensuring proper evaluation and cross-validation. Grid search is very computationally expensive. If you have two hyperparameters and three values each, then you have 3 multiplied by 3, which gives you 9, so a total of 9 candidate models that you have to train and evaluate to find the best possible one. \n",
    "\n",
    "That can get pretty intense. So you need to be aware upfront that the cost and complexity of grid search can grow very, very quickly. \n",
    "\n",
    "And if you're performing grid search to find the best candidate model on a cloud platform such as AWS, GCP or Azure, cloud-based evaluation can quickly become very expensive. \n",
    "\n",
    "Also, grid search does not differentiate between important and trivial hyperparameters. \n",
    "\n",
    "You might know up front that these hyperparameters are important and these are not, grid search does not have this information. \n",
    "\n",
    "\n",
    "An alternative to grid search for hyperparameter tuning is random search of the hyperparameter space. \n",
    "\n",
    "Specify the important parameters and randomly pick values to find the best possible candidate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which machine learning model works best on your dataset? Well, you'll never know until you evaluate a bunch of different models with different parameters, and this is where we'll use hyperparameter tuning. \n",
    "\n",
    "we'll perform hyperparameter tuning using the grid search object in scikit-learn to find the best model for our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the best model for our dataset using the GridSearchCV library in scikit-learn. Grid search is so called because it sets up the range of parameters that you specify in the form of a grid and trains a model with each combination of parameters. Once again, we'll work with the auto-mpg-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile_df = pd.read_csv('../datasets/auto-mpg-processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # never print matching warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first perform grid search to find the best parameters for our lasso regression model. Now, every regression model will have different hyperparameters that you can tune, and each model can have more than one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = automobile_df.drop(columns=['mpg','age'])\n",
    "\n",
    "Y = automobile_df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'alpha': [0.2,0.4,0.6,0.7,0.8,0.9,1.0]}\n",
    "\n",
    "grid_search = GridSearchCV(Lasso(),parameters,cv=3,return_train_score=True)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use grid search to tune just one hyperparameter, the value of alpha. \n",
    "\n",
    "Alpha is what we use to multiply the penalty terms of the lasso regression model. The alpha values that we want to test range from 0.2 to 1.0. Instantiate the GridSearchCV object with the lasso estimator. CV, as you know, stands for cross-validation. \n",
    "\n",
    "Specify the parameter dictionary for the different parameters that you want to use for alpha. \n",
    "\n",
    "Grid search will instantiate and train a lasso regression model for each value of alpha, so we have about seven values of alpha here, seven lasso regression models will be built and trained, each with a different value for alpha. \n",
    "\n",
    "The grid search object will then try and find the best model for your dataset by evaluating the model using three-fold cross validation. Your original training dataset will be split into three parts. Each model will be trained using three different runs. Two parts of the dataset will be used for training, and the third part will be used to evaluate your model. This is what cross-validation is. \n",
    "\n",
    "Your model will be scored and evaluated based on the default scoring mechanism used for that particular estimator object. In the case of regression models, the default scoring is the R square score. If you want to use grid search in more advanced ways, you can specify other scoring mechanisms to evaluate your models as well. \n",
    "\n",
    "Grid_search.fit will start a process of training on all of the different models and builds, and grid_search.best_params will return the parameters of the best model. \n",
    "\n",
    "And in the case of lasso regression here, you can see that the best value of alpha, the one which gave us the best model for our data set, is 1. But how did a value of alpha equal to 1 stack up against other models that grid search built and trained? \n",
    "\n",
    "We can get this using a simple for loop. Use a for loop to iterate through the number of models that grid search built. In this case it's 7, the number of values we had specified for alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters :  {'alpha': 0.2}\n",
      "Mean Test Score :  0.6925363169147964\n",
      "Rank:  7\n",
      "Parameters :  {'alpha': 0.4}\n",
      "Mean Test Score :  0.6953550564054829\n",
      "Rank:  6\n",
      "Parameters :  {'alpha': 0.6}\n",
      "Mean Test Score :  0.6959665194466846\n",
      "Rank:  5\n",
      "Parameters :  {'alpha': 0.7}\n",
      "Mean Test Score :  0.6960464964835806\n",
      "Rank:  4\n",
      "Parameters :  {'alpha': 0.8}\n",
      "Mean Test Score :  0.6961197952739048\n",
      "Rank:  3\n",
      "Parameters :  {'alpha': 0.9}\n",
      "Mean Test Score :  0.6961927195556029\n",
      "Rank:  2\n",
      "Parameters :  {'alpha': 1.0}\n",
      "Mean Test Score :  0.696265448479771\n",
      "Rank:  1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(parameters['alpha'])):\n",
    "    \n",
    "    print('Parameters : ',grid_search.cv_results_['params'][i])\n",
    "    \n",
    "    print('Mean Test Score : ',grid_search.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "    print('Rank: ',grid_search.cv_results_['rank_test_score'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the models that grid search built and trained, the grid_search.cv_results_ variable holds the model parameters, the mean test score, and the rank of that particular model, whether it was the best model, the second best, and so on.  \n",
    "\n",
    "And you can see here that alpha is equal to 1 produced the best model, all of the other models are listed here as well with their corresponding ranks. You can see that the worst model at rank 7 was with alpha equal to 0.2, its R square was 69.25. And here is the model at rank 1 with alpha equal to 1 with an R-square of 69.62, not that much difference here, but still enough for grid search to say that this was the best possible model. \n",
    "\n",
    "\n",
    "Once you have the best possible value for the alpha parameter, you can instantiate a lasso regression model using this value of alpha and train it on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score :  0.7113149481002754\n",
      "testing score :  0.6806665639877405\n"
     ]
    }
   ],
   "source": [
    "lasso_model = Lasso(alpha=grid_search.best_params_['alpha']).fit(x_train,y_train)\n",
    "y_pred = lasso_model.predict(x_test)\n",
    "\n",
    "print(\"training score : \", lasso_model.score(x_train,y_train))\n",
    "print(\"testing score : \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's print out the training score and the test score, and you can see that these are very close to what we got when we ran grid search. Thus, grid search is an extremely easy and convenient mechanism available in scikit-learn to perform hyperparameter tuning for your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 25}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_neighbors': [10,12,14,18,20,25,30,35,50]}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(),parameters,cv=3,return_train_score=True)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters :  {'n_neighbors': 10}\n",
      "Mean Test Score :  0.6921393882411534\n",
      "Rank:  9\n",
      "Parameters :  {'n_neighbors': 12}\n",
      "Mean Test Score :  0.6929877389878504\n",
      "Rank:  8\n",
      "Parameters :  {'n_neighbors': 14}\n",
      "Mean Test Score :  0.6958488237227393\n",
      "Rank:  7\n",
      "Parameters :  {'n_neighbors': 18}\n",
      "Mean Test Score :  0.6989496555105245\n",
      "Rank:  6\n",
      "Parameters :  {'n_neighbors': 20}\n",
      "Mean Test Score :  0.7053507002683133\n",
      "Rank:  4\n",
      "Parameters :  {'n_neighbors': 25}\n",
      "Mean Test Score :  0.7118030791078617\n",
      "Rank:  1\n",
      "Parameters :  {'n_neighbors': 30}\n",
      "Mean Test Score :  0.7074296119146241\n",
      "Rank:  2\n",
      "Parameters :  {'n_neighbors': 35}\n",
      "Mean Test Score :  0.7073293542798775\n",
      "Rank:  3\n",
      "Parameters :  {'n_neighbors': 50}\n",
      "Mean Test Score :  0.6991845408446635\n",
      "Rank:  5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(parameters['n_neighbors'])):\n",
    "    \n",
    "    print('Parameters : ',grid_search.cv_results_['params'][i])\n",
    "    \n",
    "    print('Mean Test Score : ',grid_search.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "    print('Rank: ',grid_search.cv_results_['rank_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score :  0.7305902712708826\n",
      "testing score :  0.6893821614407205\n"
     ]
    }
   ],
   "source": [
    "k_neighbors_model = KNeighborsRegressor(n_neighbors=grid_search.best_params_['n_neighbors']).fit(x_train,y_train)\n",
    "y_pred = k_neighbors_model.predict(x_test)\n",
    "\n",
    "print(\"training score : \", k_neighbors_model.score(x_train,y_train))\n",
    "print(\"testing score : \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth': [1,2,3,4,5,7,8]}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(),parameters,cv=3,return_train_score=True)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters :  {'max_depth': 1}\n",
      "Mean Test Score :  0.5123229862572404\n",
      "Rank:  7\n",
      "Parameters :  {'max_depth': 2}\n",
      "Mean Test Score :  0.6612507974874987\n",
      "Rank:  4\n",
      "Parameters :  {'max_depth': 3}\n",
      "Mean Test Score :  0.6925394732079502\n",
      "Rank:  1\n",
      "Parameters :  {'max_depth': 4}\n",
      "Mean Test Score :  0.6680105782603305\n",
      "Rank:  3\n",
      "Parameters :  {'max_depth': 5}\n",
      "Mean Test Score :  0.6718559090438972\n",
      "Rank:  2\n",
      "Parameters :  {'max_depth': 7}\n",
      "Mean Test Score :  0.6323535537036307\n",
      "Rank:  5\n",
      "Parameters :  {'max_depth': 8}\n",
      "Mean Test Score :  0.6196843121468425\n",
      "Rank:  6\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(parameters['max_depth'])):\n",
    "    \n",
    "    print('Parameters : ',grid_search.cv_results_['params'][i])\n",
    "    \n",
    "    print('Mean Test Score : ',grid_search.cv_results_['mean_test_score'][i])\n",
    "    \n",
    "    print('Rank: ',grid_search.cv_results_['rank_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score :  0.7946369482319195\n",
      "testing score :  0.7020737307060259\n"
     ]
    }
   ],
   "source": [
    "d_tree_model = DecisionTreeRegressor(max_depth=grid_search.best_params_['max_depth']).fit(x_train,y_train)\n",
    "y_pred = d_tree_model.predict(x_test)\n",
    "\n",
    "print(\"training score : \", d_tree_model.score(x_train,y_train))\n",
    "print(\"testing score : \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now there are multiple hyperparameters that you can use to design your model, and grid search can be used to train with multiple hyperparameters as well.\n",
    " \n",
    "Now, we'll train a support vector regression model using two hyperparameters, epsilon and C. Support vector regression works by trying to fit as many points as possible into a margin surrounding the best fit line, and epsilon determines the size of that margin. The parameter C, on the other hand, is the penalty that we apply to outlier points that lie outside of this margin. Grid search will train models for every combination of these hyperparameters. It will train a total of 8 models, 2 multiplied by 4. As we have many more complex models to train, this grid search takes about a minute or two on my machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.3, 'epsilon': 0.2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'epsilon': [0.5,0.1,0.2,0.3],\n",
    "             'C':[0.2,0.3]}\n",
    "\n",
    "grid_search = GridSearchCV(SVR(kernel='linear'),parameters,cv=3,return_train_score=True)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best model here was the combination where C was equal to 0.3 and epsilon was 0.2. \n",
    "\n",
    "Now that we know this combination, let's instantiate a support vector regressor using this value of C and this value of epsilon and train it on our dataset. And let's calculate the training and test score for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score :  0.7057710425121682\n",
      "testing score :  0.6928936612688478\n"
     ]
    }
   ],
   "source": [
    "SVR_model = SVR(kernel='linear',\n",
    "                epsilon=grid_search.best_params_['epsilon'],\n",
    "                C=grid_search.best_params_['C']).fit(x_train,y_train)\n",
    "y_pred = SVR_model.predict(x_test)\n",
    "\n",
    "print(\"training score : \", SVR_model.score(x_train,y_train))\n",
    "print(\"testing score : \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did fairly well. Training score of 70% and a test score of 69%. This is an example of how you'd use grid search to train multiple hyperparameters to find the best possible model. The more hyperparameters you add, the longer it takes for grid search to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
