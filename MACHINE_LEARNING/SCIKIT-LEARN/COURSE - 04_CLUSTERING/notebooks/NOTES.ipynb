{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an example of an unsupervised learning technique where we don't work with the labeled corpus to train our model. Clustering works directly with the features in your data and tries to find patterns and logical groupings in the underlying dataset. Clustering is applicable in a wide range of use cases, such as finding the relevant documents in a corpus, color quantization, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an unsupervised learning technique where you don't need labeled data in order to group or find patterns in data. K-means clustering is by far the most popular clustering algorithm; however, we'll implement and contrast different clustering techniques and figure out in what situations you might want to use those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithms seek to learn the function f that links the input features with the output labels. So you can think of supervised learning as this complex reverse engineering problem where the model is trying to figure out what exactly this f is that links the input to the output. Let's consider one of the first machine-learning models that you've probably worked with, linear regression. Linear regression involves finding the best fit line via a training process. Now linear regression is an example of supervised learning. Linear regression specifies up front that the function f connecting the input and the output is linear in nature, but really machine-learning models go from the very simple to extremely complicated. This function f could be a really complicated function, and to reverse engineer this function, you need a complex model, such as a neural network. You've probably heard that neural networks can do some pretty amazing stuff. They can learn or reverse engineer pretty much anything given then the right training data. All of the discussion that we've had so far applies only to supervised learning techniques, such as classification and regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're working with unsupervised machine-learning techniques, unsupervised learning does not have y variables or a corpus that has been labeled correctly, which means you only have raw features in your input data, and everything you do just uses those raw features. There are no labels or predictions associated with those features. So broadly, here are the two types of ML algorithms that you'll work with. Supervised learning, where the labels associated with the training data are used to correct and tweak the algorithm to build a model. In the real world though, getting labeled data is very difficult, which means you might have to work with an unsupervised learning technique. The model has to be set up right to learn hidden structures in the data. And understanding unsupervised learning techniques is very important in this context because clustering is a classic example of an unsupervised technique. When you're working with clustering or any other unsupervised learning technique, you only have the input x data, you do not have the output predictions or labels. What you are trying to do when you're working with unsupervised learning is to model or learn about the underlying structure in data to understand the data better to find patterns. Algorithms in unsupervised learning discover patterns and structure in the data by themselves. They have to be just set up right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that clustering is an example of unsupervised learning, where we set up the model to learn structure in the data. There are no labels that we use to train the model. Before we move on to talking about clustering specifically, let's talk a little bit about use cases of unsupervised learning. \n",
    "\n",
    "\n",
    "Now, it's often the case in the real world that data is unlabeled. You might apply an unsupervised learning technique to make unlabeled data self sufficient. \n",
    "\n",
    "\n",
    "For example, if you want to identify photos of a specific individual, you might feed a model lots of different photographs, millions of them until it starts identifying similar features. \n",
    "\n",
    "\n",
    "Unsupervised learning techniques are also used for latent factor analysis. Let's say you have a huge amount of data, what are the significant factors in the data? \n",
    "\n",
    "For example, finding common drivers of 200 separate stocks or shares. That's an example of latent factor analysis. \n",
    "\n",
    "\n",
    "Unsupervised learning techniques are also used for clustering.\n",
    "\n",
    "*** \n",
    "Clustering involves finding logical groupings in the underlying data. \n",
    "*** \n",
    "\n",
    "Let's say you have a large document corpus of newspaper articles, and you want to find those articles that pertain to sports. You could perform clustering on all of these newspaper articles and find that cluster which contains sports-related information. \n",
    "\n",
    "\n",
    "Unsupervised learning techniques are also used for anomaly detection. For example, in the case of credit card, you can use these techniques to flag fraudulent credit card transactions. \n",
    "\n",
    "\n",
    "Unsupervised learning techniques are also used for quantization, especially with colors. Let's say you have the original image in true color with 24-bits, and you want to compress this image before you feed it into an ML model. That's an example of unsupervised learning. \n",
    "\n",
    "\n",
    "We've seen that supervised learning techniques are very powerful, but they need labeled data, and it's hard to find labeled data in the real world. \n",
    "\n",
    "\n",
    "Unsupervised learning techniques are often used as pre-training for supervised learning problems, such as classification and regression. \n",
    "\n",
    "\n",
    "Of all of the unsupervised learning ML techniques, there are two that are very popular and widely used, autoencoders and clustering. \n",
    "\n",
    "\n",
    "So the two most popular unsupervised ML algorithms are clustering, which is used to identify patterns in data. These patterns are then used to bring together these data items into logical groups. \n",
    "\n",
    "\n",
    "K-means clustering is a popular and widely-used clustering algorithm, and that's what we'll study first in this module. \n",
    "\n",
    "*** \n",
    "Autoencoding is another example of a popular unsupervised ML algorithm. Autoencoders are typically used to identify latent factors in the underlying data. \n",
    "*** \n",
    "\n",
    "Let's say you have a very large dataset, and this dataset could be essentially anything. They could be documents, they could be people, they could be events, how do you find patterns in this data, how do you make sense of this very large dataset? \n",
    "\n",
    "\n",
    "One intuitive way is to group these data items based on some common attributes. So data items that belong to the same group have something in common, and data items that are in different groups are different in some way. And this is exactly what clustering tries to do. \n",
    "\n",
    "\n",
    "Now clustering is easy to imagine when you're talking about simple data, maybe just coordinates, but what if you want to cluster more complex data such as products sold on Amazon, people on Facebook, or websites indexed by Google. \n",
    "\n",
    "\n",
    "You can imagine that each of these entities can be very complex. There are a rich set of attributes or information available about these entities. How do you work with these to cluster them into logical groups? Too many entities, too many attributes per entities, and many of these attributes are not really numeric, and all of these put together results in huge complexity. This is a difficult problem to solve. \n",
    "\n",
    "\n",
    "Now you might already know that machine-learning models can accept only numeric input. The interesting thing is that any attribute belonging to any entity can be represented by a set of numbers. \n",
    "\n",
    "\n",
    "So for a product, some of the numbers that you use to represent it could be the product ID, the timestamp it was sold, and the amount a customer paid. \n",
    "\n",
    "\n",
    "For a person, it could be age, height, weight, how frequently does she hit like on a particular kind of news, how frequently she logs on to Facebook or your social media site. \n",
    "\n",
    "\n",
    "For a web page, you could have the length of a web page or word frequencies within a web page. These could be your numbers to represent that page. \n",
    "\n",
    "\n",
    "Once you have data represented in the form of numbers, every entity here, whether it's a product, a user, or a web page can be represented using a feature vector. And every entity is a data point, that is the representation of this feature vector. For example, age, height, and weight of a person can be plotted in a 3-dimensional plane. You need just one dimension for age, we add another dimension for height, and finally when we add in a third dimension, let's say the weight of an individual, we add in a third axis, the Z axis. \n",
    "\n",
    "\n",
    "Just extend your imagination to more complex features that can be represented using numbers and to a dimension beyond 3. So a set of N numbers represents a point in an N-dimensional hypercube. And once you have points in this N-dimensional hypercube, you can perform clustering and grouping using different techniques. \n",
    "\n",
    "\n",
    "You can use distance measures to compute the distances between these points to find which points lie in the same cluster, or you could find regions of very high density, and those could be your clusters. These are all examples of different clustering techniques that you could employ. \n",
    "\n",
    "\n",
    "\n",
    "Let's say you wanted to perform clustering on a set of points where every data point represents a Facebook user. Once clustering has been performed, people in the same group are similar to one another, and people who are in different groups or different clusters are different from one another. Based on the attributes that you've looked at to perform this clustering or the clustering model that you've used, your groupings could be different. \n",
    "\n",
    "\n",
    "Now in a real-world use case, it's possible that users who are in the same cluster may like the same kind of music, may have gone to the same high school, they have the same friends, or may enjoy the same kinds of movies. \n",
    "\n",
    "\n",
    "If you want to run ad campaigns on individuals who have the same taste, here is one way to do it. \n",
    "\n",
    "\n",
    "All of the data points in this N-dimensional hypercube are separated by a distance, the distance between users, how similar or different they are. Distances between users who are in the same cluster should be small, that is users in the same cluster should be similar to one another, and distances between users in different clusters should be large. These are the objectives of clustering. \n",
    "\n",
    "*** \n",
    "Entities in the same group should be very similar, and entities in different groups should be very different. \n",
    "\n",
    "*** \n",
    "#### What are some of the other use cases of clustering? \n",
    "\n",
    "\n",
    "To find relevant documents in a corpus. Now document archives are generally very rich, and it's hard to identify content relevant to a specific user or query. It's not possible for all of these documents to have labels or keywords. You can clump documents into semantically-similar groups using clustering. \n",
    "\n",
    "\n",
    "Clustering is also used for color quantization, where we represent the original image using fewer distinct colors. True color images represent each image using 24-bits per pixel, which is huge. Many displays and image formats cannot use this kind of granularity, they use just 8-bits per pixel. Just randomly picking 2 to the power 8, that is 256 colors to represent the original image is not optimal. If your original image is of the sea, there'll be too few colors in your color set to represent this image. This is where you can use clustering to identify the 250 most representative colors for your image, and then you'll quantize each true color to the nearest shade or to the nearest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of clustering is to maximize intra-cluster similarity and to minimize inter-cluster similarity, and that's exactly what the K-means clustering algorithm tries to achieve. \n",
    "\n",
    "\n",
    "In order to understand how K-means clustering works, let's imagine data in two dimensions. To perform K-means clustering, we first initialize K centroids, that is means in this data. Now these K values can be any values picked at random, or there are algorithms that you can use to choose these K centroids that we start off with. \n",
    "\n",
    "\n",
    "Once we have these K centroids, iterate through all of the remaining points and assign each point to a cluster. You'll measure the distance between the centroid and all of the points, and points that are close to a particular centroid will be assigned to the cluster represented by the centroid. \n",
    "\n",
    "\n",
    "Once we have all of the clusters, you will recalculate the mean or the centroid for all of these clusters. At this point, your centroid or mean values might move a little bit. \n",
    "\n",
    "\n",
    "Once we have the new mean values, you will then reassign the points to clusters that are closest to those points. And this is the same process that we apply over and over again. \n",
    "\n",
    "\n",
    "Iterate until all of the points are in their final clusters and your means no longer move. And this is how you know your algorithm has terminated, this is where your K-means algorithm reaches convergence. \n",
    "\n",
    "\n",
    "K-means clustering is an example of a centroid-based clustering algorithm, where every cluster can be represented using a centroid or a reference vector. The term reference vector makes more sense, but because of how we calculate these means by finding the average of all of the points in a cluster, these reference vectors are often called centroids. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the pseudocode for K-means clustering looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../files/Capture7.png\" width=\"200\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very first step we have initialize K centroids, this is an initial solution, there are algorithms that exist to pick these initial centroids well, so that your clustering performs well. \n",
    "\n",
    "\n",
    "The next two sets of actions you perform until convergence, until your cluster centers no longer move. \n",
    "\n",
    "\n",
    "You'll iterate through all of the points that you have in your dataset, and for each data point, you will assign that data point to the nearest cluster by calculating the distance of that point from all of your centroids and finding the nearest one. \n",
    "\n",
    "Once all of your points have been assigned to some cluster, you will then update the coordinates of the centroid or the reference vector. \n",
    "\n",
    "You'll find new coordinates by averaging all of the data points that belong to that cluster. \n",
    "\n",
    "After every iteration, you'll check to see whether the reference vectors or the centroids have moved. If they move, the algorithm hasn't converged, if the centroids have converged, we are done, we stop iterating. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Each step of this process involves a design parameter in K-means clustering. These are the hyperparameters of your model. \n",
    "\n",
    "When you initialize K centroids, you have to specify the value of K, that is the number of clusters into which you want to group your data. \n",
    "\n",
    "The initial value for these centroids can be randomly chosen or there are algorithms that you can use to pick these values. \n",
    "\n",
    "In the next step, you have to calculate for each data point which centroid or cluster center is the closest. \n",
    "\n",
    "The distance measure is also a hyperparameter. Euclidean distance is the one that is most commonly used, and that's the distance measure that we use when we use our scikit-learn estimator. \n",
    "\n",
    "The third design decision for this algorithm is how we update the coordinates of the cluster center. Calculating the cluster center from points in the cluster can be done in different ways, the simple averaging technique is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Clustering Models\n",
    "\n",
    "\n",
    "Once you've applied a clustering technique or a model to your data, how do you know that the clustering that you've performed is good? There are different techniques that you can use to evaluate your clustering models. Once you have clusters in your data, you can calculate the homogeneity score, the completeness score, the V-measure score, the Adjusted Rand Index, Adjusted Mutual Info score, and the silhouette score for your clusters. \n",
    "\n",
    "\n",
    "Of all of these scoring techniques, only the silhouette score can be computed without original categories or labels on your data. \n",
    "\n",
    "\n",
    "All of the other scores require labeled data. Once you get a conceptual understanding of what exactly these scores are trying to capture, you'll find that scikit-learn has built-in functions for calculating all of these scores, so implementation is very straightforward. \n",
    "\n",
    "\n",
    "Let's start off by discussing homogeneity, completeness, and the V-measure score. You'll find that these are closely related. \n",
    "\n",
    "\n",
    "We'll first discuss homogeneity and completeness. Both of these are properties that we want in our clusters. \n",
    "\n",
    "\n",
    "Homogeneity basically says that every cluster should contain entities or members that belong to the same class. So within a cluster, if you have entities which belong to a different class, your homogeneity score will be lower. \n",
    "\n",
    "\n",
    "On the other hand, the completeness measure says that all members which belong to a particular category or a class should lie in the same cluster.\n",
    "\n",
    "\n",
    "So they are subtly different and this difference is important. Now this subtle difference becomes quite significant, and you'll find that homogeneity and completeness are inversely related. So when you try to increase homogeneity, you'll find that completeness might fall. \n",
    "\n",
    "\n",
    "Each is a separate score that lies between 0 and 1. Higher values are obviously better. Now if you've worked with classification algorithms, you know that precision and recall are two metrics that we use to evaluate classifiers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The term homogeneous is different from completeness in the sense that while talking about homogeneity, the base concept is of the respective cluster which we check whether in each cluster does each data point is of the same class label. While talking about completeness, the base concept is of the respective class label which we check whether data points of each class label is in the same cluster.\n",
    "\n",
    "\n",
    "<img src=\"../files/Capture9.png\">\n",
    "\n",
    "\n",
    "In the above diagram, the clustering is perfectly homogeneous since in each cluster the data points of are of the same class label but it is not complete because not all data points of the same class label belong to the same class label.\n",
    "\n",
    "\n",
    "<img src=\"../files/Capture10.png\">\n",
    "\n",
    "\n",
    "In the above diagram, the clustering is perfectly complete because all data points of the same class label belong to the same cluster but it is not homogeneous because the 1st cluster contains data points of many class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity and completeness are similar to precision and recall, and we need a third metric to optimize the tradeoff between these two. And this third metric that will allow us to figure out an optimal value for homogeneity and completeness is the V-measure score, and here is the mathematical formula for the V-measure. This is the harmonic mean of homogeneity and completeness. \n",
    "\n",
    "\n",
    "<img src=\"../files/Capture8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harmonic mean is often closer to the lower of the two values, homogeneity and completeness, and it favors an even weightage of both metrics. \n",
    "\n",
    "\n",
    "When you're evaluating your clustering models, it's helpful to take into account homogeneity, completeness, and the V-measure scores all together. Because they are a related set of metrics, all of these are bounded scores between 0 and 1. You'll find that these are the most commonly-used metrics to evaluate your clustering models because they happen to be easy to interpret. \n",
    "\n",
    "\n",
    "You just know that higher values are better, and the these can be applied to any clustering algorithm. One drawback is you require labeled data. All of the data that you're clustering need to have the original classes or categories to which they belong in order to calculate these scores.\n",
    "\n",
    "\n",
    "Another metric that you can use to evaluate how well your clustering model performed is the Adjusted Rand Index, or ARI. This metric tries to measure the similarity between the original labels assigned to your data and the clusters into which your data points have been grouped. Once again this is a metric that can be calculated only if you have labeled data available. \n",
    "\n",
    "\n",
    "The term adjusted here is because it's adjusted for the probability of correct labeling purely by chance. So if you were to randomly take your data and assign them to clusters, what is the probability that you'll end up with this set of clusters? This is named after William Rand, that's why it's called the Adjusted Rand Index. \n",
    "\n",
    "\n",
    "When you use scikit-learn's function to calculate the Adjusted Rand Index, you'll get a value between -1 and 1. A value of 1 indicates that the labels and the predicted clusters from your clustering model agree perfectly. Your clustering model did very well. If you get 0 or negative values, that's indicative of bad clustering. This indicates that the labels and the calculated clusters are independent, there is no relationship between the two. \n",
    "\n",
    "\n",
    "Let's now move onto discussing another evaluation metric, the Adjusted Mutual Information. This is a measure of the mutual information in the overlap between cluster assignments, and once again like the other techniques that we've seen, needs labeled data. \n",
    "\n",
    "\n",
    "Mutual information between two labels is a measure of how well you can predict one label by seeing variations in another. An Adjusted Mutual Information score of 1 is the highest possible value, that means your clusters have been formed well. Actual labels and predicted labels match, 0 or negative values are bad, and this indicates assignments of data points to clusters have been done at random. The labels and calculated clusters are independent. \n",
    "\n",
    "\n",
    "And finally, before we move on, let's briefly talk about the silhouette score.\n",
    "\n",
    "\n",
    "An important advantage of using silhouette scoring to evaluate your cluster model is the fact that you do not need labeled data. Silhouette scoring just works on the features. \n",
    "\n",
    "\n",
    "Silhouette score involves the calculation of something known as the silhouette coefficient, and the silhouette coefficient is associated with each sample in your dataset. \n",
    "\n",
    "The silhouette coefficient is a measure of how similar an object is to objects in its own cluster, and how different an object is from objects which live in other clusters. \n",
    "\n",
    "The overall silhouette score for your clustering model averages the silhouette coefficient for each sample. Since we are only looking at data points, there is no need for labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories of Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "four broad categories into which clustering techniques are divided, centroid-based, hierarchical, density-based, and distribution-based clustering models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first talk about clustering or grouping data, the logical place to start is k-means clustering. That's because the k-means clustering algorithm is intuitive and easy to understand. And it also has great performance. However, as you work with data in the real world you'll find that the notion of a cluster cannot be precisely defined, which is why there are so many clustering algorithms. \n",
    "\n",
    "K-means clustering that we looked at in detail  is an example of centroid-based clustering. There are other categories of clustering algorithms used by data researchers and scientists such as hierarchical clustering, distribution-based clustering, and density-based clustering. \n",
    "\n",
    "\n",
    "All of these clustering models serve to find groupings of data objects. So the objective is the same, the techniques are different. And these different techniques are based on different notions of what exactly a cluster is.\n",
    "\n",
    "K-means clustering is the most popular and widely used example of a centroid-based clustering algorithm. Here, the cluster is represented by a central reference vector. And this reference vector may not be part of the original dataset. \n",
    "The exact optimization problem underlying these clustering models are often really hard to solve, which is why we look for approximate solutions. Such clustering algorithms typically use distance measures to assign points which are close to a particular cluster center to that cluster. \n",
    "\n",
    "\n",
    "Hierarchical clustering models are another commonly-used technique. This is a connectivity-based clustering model based on the idea that objects are related to other objects which are closer by, than two objects which are further away. There is no central mean or reference vector to which distances are calculated. Points are compared with one another. And points that are closer together are clumped together. \n",
    "Clusters formed using hierarchical clustering techniques can be defined largely by the maximum distance needed to connect different parts of the cluster. Points which are within this maximum distance belong to the same cluster. Unlike centroid-based clustering algorithms, such as k-means, that look to partition the dataset into clusters, hierarchical clustering algorithms construct a tree of points where the leaf nodes of the tree are then merged together to form clusters. Examples of hierarchical clustering techniques that we'll work with in this course are agglomerative clustering and BIRCH clustering. \n",
    "\n",
    "\n",
    "\n",
    "Another category of clustering models is distribution-based clustering. These are based on statistical distribution models. Objects within a cluster are those data points which belong most likely to the same distribution. They are drawn from the same distribution. A very convenient feature of this approach is that this closely resembles the way artificial datasets are generated by sampling random objects from a particular distribution. \n",
    "These clustering models are not very easy to use though, and they are prone to a problem known as overfitting. Unless the clustering model parameters are constrained using some kind of regularization techniques here you'll get very complex models, and they'll generally tend to explain the underlying data better. But these might be overfitted models which work well on the existing dataset. And if you then try to apply your cluster, say for prediction, these clusters may not perform so well. An example of distribution-based clustering models is Gaussian mixture models. \n",
    "\n",
    "\n",
    "\n",
    "Another category of clustering models can be defined as density-based models. Here, the notion of a cluster refers to those areas where data points are at a very high density as compared to other areas. Density-based models seek to create clusters from areas that have a higher density of data points. Now, in the real world you might have very high density areas which are separated by areas which are sparse. Objects which are located in these sparse areas with separate clusters are considered noise or border points.  two specific examples of density-based clustering are, DBSCAN and mean-shift clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Right Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find that using scikit-learn's estimator API, it's very easy to implement a particular kind of clustering on your data. What is a little more nuance and what needs a little bit of understanding is choosing the right clustering algorithm based on your use case and understanding the parameters that you'll use to tweak and design that algorithm. Followin are  some high-level principles that you can use to choose the right clustering algorithm. \n",
    "\n",
    "\n",
    "Let's set this up in a matrix form. You'll choose a clustering algorithm based on two important factors, the number of clusters you want to form in your data and the size of the original dataset. Now for number of clusters, let's divide this into three broad categories. \n",
    "\n",
    "\n",
    "<img src=\"../files/Capture20.png\">\n",
    "\n",
    "You want many clusters, you want a moderate number of clusters, or you want just a few clusters in the underlying data. \n",
    "\n",
    "\n",
    "The rows of this matrix represent these three categories. Another principle is to consider the size of the dataset that you are working with. You could have a very small dataset, anything under 1000 rows would be considered small, or you have a medium-sized dataset, or you're working with a very large dataset with millions of records. \n",
    "\n",
    "\n",
    "Each column in this matrix corresponds to the three categories. \n",
    "\n",
    "\n",
    "Now, let's say you have a very large dataset with millions of records and you want to categorize the data in this set into many clusters. The clustering algorithm that you might choose to use are either the BIRCH or agglomerative clustering algorithms. BIRCH is an acronym, and we'll see exactly what it stands for when we study it in detail. Both BIRCH and agglomerative clustering are examples of hierarchical clustering algorithms. This is the category of connectivity-based algorithms where points form a cluster when they are closer together. Both of these algorithms work by building a tree representation of the underlying data. Every data point forms a leaf of this tree, and these leaves are then merged together into different numbers of clusters. Once you have this tree representation, you can say all data points that belong to this particular branch of this tree form a cluster. Both of these clustering algorithms happen to be very scalable. They work well when we have a very large dataset and you want to cluster these into a large number of clusters. They also work well --- BIRCH clustering uses Euclidean distance between points in order to see which points belong to a cluster. It's also great for detecting and removing outliers in your dataset. Such outlier data will be classified as noise by our BIRCH algorithm. The BIRCH algorithm can also be used to cluster incoming data streams in an incremental manner. You can incrementally process the incoming data and update clusters with BIRCH. The one disadvantage of BIRCH is that it doesn't really scale very well to high dimensional data, data where every record has a large number of features. \n",
    "\n",
    "\n",
    "\n",
    "Agglomerative clustering can work even in the absence of Euclidean distance unlike, say, BIRCH clustering. Agglomerative clustering merges clusters in a tree representation using the bottom-up approach. Leaf data points are successfully merged together to form larger clusters. Agglomerative clustering can scale to large numbers of samples. However, this can get computationally expensive, so that's something you need to watch out for. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If you want to generate many clusters in the underlying data, but your dataset size is small, you'll choose to go with mean-shift clustering or affinity propagation clustering. Both of these clustering models belong to the category of density-based clustering. \n",
    "\n",
    "This is where the cluster is defined as an area with a large number of data points. Remember that both of these clustering techniques are not very scalable. They work with small datasets when you want to generate a large number of clusters. \n",
    "\n",
    "You'll prefer to use these algorithms when you want many clusters of uneven size and your dataset is such that it has a non-flat geometry. That is, your dataset is in the form of a manifold shape. This is a shape which is simple in a lower dimension which has been twisted and rolled and turned and is a more complex shape in a higher dimension. \n",
    "\n",
    "Mean-shift clustering uses the pairwise distances between points in order to form clusters. Because distances are calculated pairwise, this is why this tends to be computationally intensive and not very scalable. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Affinity propagation uses graph distance, maybe a nearest-neighbor graph, in order to figure out which points are closer together. One advantage of affinity propagation is that it does not need the number of clusters to be specified up front. If you have a very large dataset with many, many points, maybe millions, and you want to cluster them into a moderate number of clusters, you'll choose to use k-means clustering or DBSCAN clustering. So if you have a very large dataset, but you want it partitioned only into a moderate number of clusters here are two choices for you. And each have their own strengths and weaknesses. \n",
    "K-means clustering works very well when you want the resulting cluster sizes to be fairly even. You want every cluster to have almost the same number of data points. If your original data forms a flat surface and it's not really a complex curve in higher dimensions k-means is great. You can get even better performance with k-means clustering if you use mini-batch k-means. This is a tweak on the original k-means algorithm that we studied earlier. The algorithm is much faster, much more performant, and almost as good, not perfect. You'll choose DBSCAN clustering if your data is in the form of manifolds, that is a complex curve in higher dimensions, and if you're okay with uneven cluster sizes of the resulting clusters.\n",
    "\n",
    "\n",
    "\n",
    "If you're working with medium-sized data that you want grouped into just a few clusters, a clustering algorithm that you might choose is spectral clustering. This is a good clustering algorithm for datasets that are small to medium sized and you want a small number of clusters. This is extremely simple to implement, and it's based on graph distance. \n",
    "You construct a nearest-neighbors graph to figure out which points belong together in the same cluster.\n",
    "You'll get intuitive results for data exploration. \n",
    "\n",
    "The clusters that you get have relatively even sizes. And spectral clustering even works well with manifold data. This is data where a simple manifold has been twisted, turned, and folded into a complex shape at higher dimensions. The underlying algorithm relies on the distance between points to set up the nearest-neighbors graph. And here you see it in one matrix, all of the different clustering algorithms you can choose based on the size of the dataset and the number of clusters you want to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "\n",
    "At this point, we have a good understanding or centroid-based clustering when we looked at the k-means clustering example. Let's get the same understanding of hierarchical clustering before we move on. Here we have t data points. Let's say we were to apply hierarchical clustering. We would start off with t clusters where every cluster would have exactly one point. Hierarchical clustering is connectivity- based clustering. The dataset is not partitioned, instead points are merged together into clusters. So t clusters each are one point. The algorithm will then go ahead and merge two clusters that are the closest together. So two clusters come together to form one. So we now have t-1 clusters. There's exactly one cluster here with two points. And this same process is repeated over and over again until the different points are merged together to form a single large cluster. So, merge together two points which are the closest together. So we now have t-2 clusters, two of these with two points. And continue this process until we have fewer and fewer numbers of clusters. As this process continues, the number of clusters keeps reducing. At this point in time, here in our visual we have exactly two clusters, each with multiple points. Let's merge these two clusters together, and we are left with exactly one cluster with all of the t points belonging to that cluster. If you think about it, what you just saw was a tree-based representation of all of the data points. This tree diagram used to illustrate the arrangement of clusters produced by hierarchical clustering is called a dendrogram. Let's see a simpler example of hierarchical clustering with just a few levels so that you can see how this dendrogram structure looks like. Here we have 10 clusters, each with exactly 1 point. Let's color these points so that points which have the same color belong to the same clusters. So we have four clusters here, each having a varying number of points. I'll now move these data points around so that points belonging to the same cluster are arranged together. As we rinse and repeat this process, at some point all of these points will come together to form a single large cluster which contains all of the original 10 points. Now let's set up this dendrogram representation. Here are the original 10 points, each in its own cluster. Consider these points the leaf nodes of the tree diagram representation. Now, these leaf nodes get merged together to form internal nodes of our dendrogram representation exactly as you see here on screen. These are the four clusters which formed an intermediate stage. And now these four clusters with varying numbers of points are once again merged together and at one point will form a single large cluster containing all of the 10 points. The interesting thing about this dendrogram, or tree diagram representation, is that you get a lot of levy in how you want to partition the underlying data into clusters. You can simply choose a different branch of the tree to vary the number of clusters you want in your data. There are two broad ways in which hierarchical clustering can be performed on data. The example that we just saw was an example of agglomerative clustering. We start with many one-point clusters and end up with one big cluster. You can also implement hierarchical clustering in a divisive manner. We start with one big cluster and divide that into sub clusters and end up with many one-point clusters. The scikit-learn library offers an estimator API for agglomerative clustering. And that's what we'll use in the next clip. But before that, let's quickly compare and contrast k-means clustering and hierarchical clustering. Both of these clustering types use distance measures to figure out which points belong together. In the case of k-means clustering, you need a way to aggregate points in a cluster. In hierarchical clustering, you do not need a way to aggregate these points. The distance measure is sufficient. In the case of k-means clustering, you need to represent data as vectors in an N-dimensional hyperspace or hypercube. Hierarchical clustering does not use this concept of having data represented in an N-dimensional hyperspace. This need to represent data in the form of a vector for k-means clustering means that data representation can be difficult when you're working with complex data types such as, say, text. Whereas with hierarchical clustering, it's relatively simple to represent even complex data types such as graphs or documents. K-means clustering is actually very, very scalable to large datasets. There are many variants such as mini-batch k-means and the BFR variant that can efficiently deal with very large datasets on disk. Now with hierarchical clustering, even with careful construction it might prove to be too computationally expensive for large datasets stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
