{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "This is the neural network you have seen before. To ensure we all on the same page, let's walk through it from input to output and review structures and terminology. On the left we have the input layer, which feeds data into the network. This data could be values from a data table, images from a camera, sounds from a recording, or output from a sensor. The input layer does not change the data, it simply passes it for processing by the remaining layers. The data from the input layer is passed to another layer of neurons. These layers can be of different types with the different layer types performing different transformations on the data as required by our solution. In a simple network, the input layer can be directly connected to an output layer of neurons, which provide the final outputs. But in most networks, the input layer is connected to hidden layers. Hidden layers are defined as not being input or output layers, and therefore, are hidden to the code that's using the neural network. The hidden layers we define will perform many interesting transformations, which we will see in later modules. For now, remember they do what we define them to do. So now we have the basic form of a neural network, but remember, this is a class about deep learning, just where does that deep learning term come from? The network can have many hidden layers and if there are two or more hidden layers we call the network a deep neural network. In this course, we are focusing on these deep neural networks and their learning process, which is known as deep learning. In this learning process we adjust the structures of our neurons. To understand this, let's look at a single neuron. Here we see the inputs and outputs we showed in the network diagram, with the inputs going in and the outputs going out. However, what is not showing in this diagram is how the neurons work internally. To do that, we need to open the neuron so we can see how it is constructed. As we see, the neuron is performing a simple mathematic summing of weights times the input value and adding a bias. The product of these operations is passed through a nonlinear activation function. And the output of the activation function is the output of the neuron. A key feature of the neural network is the ability to use the input data to train the weights and biases so the signal passed out of the neuron changes based on the input data. To do this training, we expose the network to data. With each set of data, an algorithm is used to adjust the weights and bias to minimize the error the network has in predicting the data's values. This is done through processes called forward propagation and back propagation. And when these processes are complete, the network is said to be trained, and the weights and biases of all the neurons have been adjusted to give the best results on the training data. So to summarize, the neural network consists of three layer types, input, hidden, and output. By passing training data through the network, the network of neurons is trained to give results with the least error. This training is done by adjusting weights and biases in the neurons, utilizing the processes of forward propagation and back propagation, and if there are two or more hidden layers in the network, we say it's a deep neural network. Now that we all have the same background in neural networks, let's talk about the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as kbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(pl,x,y):\n",
    "    pl.plot(X[y==0,0],X[y==0,1],'ob',alpha=0.5)\n",
    "    pl.plot(X[y==1,0],X[y==1,1],'xr',alpha=0.5)\n",
    "    pl.legend(['0','1'])\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
