{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\\\apache-spark\\\\spark-3.0.0-bin-hadoop2.7')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.10, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_241\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2020-06-06T11:32:25Z\n",
      "Revision 3fdfce3120f307147244e5eaf46d61419a723d50\n",
      "Url https://gitbox.apache.org/repos/asf/spark.git\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application.\n",
    "\n",
    "Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appName parameter is a name for your application to show on the cluster UI. master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass “local” to run Spark in-process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262]\n",
      "[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262]\n",
      "[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262]\n",
      "[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:262]\n",
      "[ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:262, ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:262]\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:19\n",
      "-------------------------------------------\n",
      "(8, 100)\n",
      "(0, 99)\n",
      "(1, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:20\n",
      "-------------------------------------------\n",
      "(8, 100)\n",
      "(0, 99)\n",
      "(1, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:21\n",
      "-------------------------------------------\n",
      "(8, 100)\n",
      "(0, 99)\n",
      "(1, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:22\n",
      "-------------------------------------------\n",
      "(8, 100)\n",
      "(0, 99)\n",
      "(1, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:23\n",
      "-------------------------------------------\n",
      "(8, 100)\n",
      "(0, 99)\n",
      "(1, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(3, 100)\n",
      "(4, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-08-10 15:38:27\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"SampleQStream\")\n",
    "    ssc = StreamingContext(sc,1)\n",
    "#     Main entry point for Spark Streaming functionality. It provides methods used to create DStreams from various input sources. \n",
    "    \n",
    "    rddQueue = []\n",
    "    for i in range(5):\n",
    "        rddQueue+= [ssc.sparkContext.parallelize([j for j in range(1,1000)],10)]\n",
    "#         print(rddQueue)\n",
    "        \n",
    "    inputStream = ssc.queueStream(rddQueue)\n",
    "    mappedStream = inputStream.map(lambda x:(x % 10,1))\n",
    "    reducedStream = mappedStream.reduceByKey(lambda a,b:a+b)\n",
    "    reducedStream.pprint()\n",
    "    \n",
    "    \n",
    "    ssc.start()\n",
    "    time.sleep(6)\n",
    "    ssc.stop(stopSparkContext=True,stopGraceFully=True )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main entry point for Spark Streaming functionality. A StreamingContext represents the connection to a Spark cluster, and can be used to create DStream various input sources. It can be from an existing SparkContext. After creating and transforming DStreams, the streaming computation can be started and stopped using context.start() and context.stop(), respectively. context.awaitTermination() allows the current thread to wait for the termination of the context by stop() or by an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "queueStream(rdds, oneAtATime=True, default=None)[source]\n",
    "Create an input stream from an queue of RDDs or list. In each batch, it will process either one or all of the RDDs returned by the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize() method is the SparkContext's parallelize method to create a parallelized collection. This allows Spark to distribute the data across multiple nodes, instead of depending on a single node to process the data\n",
    "# parallelize(c, numSlices=None)\n",
    "# Distribute a local Python collection to form an RDD.\n",
    "# for i in range(5):\n",
    "#     print(ssc.sparkContext.parallelize([j for j in range(1,1000)],10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "Action − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
